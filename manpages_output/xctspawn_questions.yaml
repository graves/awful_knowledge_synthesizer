- prompt: "You are playing the role of a college professor. Here is some text copied from the manpages of the macOS resource `xctspawn`.\n\n\n\nManpage text:\n\nxctspawn(1)\t\t    General Commands Manual\t\t   xctspawn(1)\n\n\n\nNAME\n       xctspawn - Command line utility to execute XCTest unit and UI tests\n       locally on device.\n\n\nSYNOPSIS\n       xctspawn /path/to/test_run_file.xctestrun [options]\n\n\nDESCRIPTION\n       To run your tests, point the tool at the xctestproducts file or the\n       xctestrun file sitting alongside your built tests. The tool will\n       execute the tests and generate an Xcode result bundle containing the\n       test results (pass/fail/skipped/xfail status, activities, attachments,\n       crash reports, etc.). Note that the tests must be built for the\n       platform you are running them on.\n\n       If this tool is interrupted via SIGINT while tests are still executing,\n       it will still generate a result bundle containing the test results that\n       have been produced so far.\n\n\nOPTIONS\n       --result-bundle-path /path/to/generated_result_bundle.xcresult\n\t      Location that the generated result bundle should be placed. By\n\t      default, the result bundle is emitted to the current working\n\t      directory.\n\n       --result-stream-path /path/to/result_stream.txt\n\t      A path to a file or socket where events will be emitted during\n\t      test execution.  Events include \"suite started\", \"suite\n\t      finished\", \"test started\", \"test finished\", etc.\tThe events are\n\t      JSON-formatted XCResultKit types. For more information about the\n\t      event stream, see\n\t      https://confluence.sd.apple.com/display/DT/Using+xcodebuild%27s+event+stream\n\n       --only-test-configuration configuration\n\t      Constrains testing by specifying test configurations to include,\n\t      and excluding other test configurations.\n\n       --skip-test-configuration configuration\n\t      Constrains testing by specifying test configurations to exclude,\n\t      but including other test configurations.\n\n       --only-testing test identifier\n\t      Constrains testing by specifying tests to include, and excluding\n\t      other tests. The test identifier takes the form\n\t      `TestTargetName[/TestClassName][/TestMethodName]`, e.g.\n\t      `MyTestTarget/MyTestClass/testFoo` (ObjC) or\n\t      `MyTestTarget/MyTestClass/testFoo()` (Swift).\n\n       --skip-testing test identifier\n\t      Constrains testing by specifying tests to exclude, but including\n\t      other tests. The test identifier takes the form\n\t      `TestTargetName[/TestClassName][/TestMethodName]`, e.g.\n\t      `MyTestTarget/MyTestClass/testFoo` (ObjC) or\n\t      `MyTestTarget/MyTestClass/testFoo()` (Swift).\n\n       --test-plan\n\t      Specifies which test plan in the xctestproducts to run.\n\n       --parallel-testing-enabled YES | NO\n\t      Overrides the per-target setting in the scheme/test plan.\n\n       --parallel-testing-worker-count number\n\t      The exact number of test runners that will be spawned during\n\t      parallel testing.\n\n       --parallel-testing-maximum-worker-count number\n\t      The maximum number of test runners that will be spawned during\n\t      parallel testing.\n\n       --test-timeouts-enabled YES | NO\n\t      Enable or disable test timeout behavior.\n\n       --default-test-execution-time-allowance seconds\n\t      The default execution time an individual test is given to\n\t      execute, if test timeouts are enabled.\n\n       --maximum-test-execution-time-allowance seconds\n\t      The maximum execution time an individual test is given to\n\t      execute, regardless of the test's preferred allowance.\n\n       --trace-collection-enabled YES | NO\n\t      Whether to collect a ktrace/artrace file during the execution of\n\t      performance tests.\n\n       --enable-performance-tests-diagnostics YES | NO\n\t      Whether to collect a memgraphset file during the execution of\n\t      performance tests.\n\n       --performance-test-configuration\n       /path/to/performance_test_configuration\n\t      The path to a performance test configuration file that controls\n\t      how performance tests are executed.\n\n       --test-iterations number\n\t      The number of times to execute each test, or, if either\n\t      --run-tests-until-failure or --retry-tests-on-failure are\n\t      specified, the maximum number of times each test will be\n\t      executed.\n\n       --run-tests-until-failure\n\t      Run each test until it fails, up until a maximum number of\n\t      iterations. The maximum defaults to 100, but can be customized\n\t      via the --test-iterations option.\n\n       --retry-tests-on-failure\n\t      Re-run a failing test until it succeeds, up until a maximum\n\t      number of iterations. The maximum defaults to 3, but can be\n\t      customized via the --test-iterations option.\n\n       --test-repetition-relaunch-enabled YES | NO\n\t      Enable or disable tests repeating in a new process for each\n\t      repetition. Must be used in conjunction with --test-iterations,\n\t      --retry-tests-on-failure, or --run-tests-until-failure. If not\n\t      specified, tests will repeat in the same process.\n\n       --skip-app-reinstallation\n\t      Don't install an app (either a test runner app or a UI target\n\t      app) from the build products if there is already a copy of the\n\t      app installed on the system. If there isn't a copy on the\n\t      system, the app will still be installed.\n\n       --disable-code-coverage\n\t      If the tests were built with code coverage enabled, this option\n\t      will skip the generation of a coverage report.\n\n       --arg value\n\t      A command-line argument to supply to any test runner process\n\t      that is launched during test execution. Can be specified\n\t      multiple times.\n\n       --env value\n\t      A key-value pair of the form EnvVar=Value to supply as an\n\t      environment variable to any test runner process that is launched\n\t      during test execution.\n\n       --enumerate-tests\n\t      If specified, the set of tests that would normally execute will\n\t      instead be listed/enumerated, and the list of tests will be\n\t      output to either stdout (the default), or to a file whose\n\t      location is specified via the --test-enumeration-output-path\n\t      option. The format of the list of tests is controlled via the\n\t      --test-enumeration-style and --test-enumeration-format options.\n\n       --test-enumeration-style hierarchical | flat\n\t      Whether tests should be enumerated in a hierarchical\n\t      organization (the default), meaning grouped by test plan,\n\t      target, and class, or as a flat list of test identifiers that\n\t      can subsequently be passed to the --skip-testing and --only-\n\t      testing options.\n\n       --test-enumeration-format text | json\n\t      Whether tests should be enumerated as human-readable text (the\n\t      default), or as machine-parseable JSON.\n\n       --test-enumeration-output-path path | -\n\t      Specifies a file path where the list of tests computed by the\n\t      --enumerate-tests option will be written to disk. If - is\n\t      supplied, the data will be written to stdout (which is also the\n\t      default if this option is omitted).\n\nApple Inc.\t\t\t     2021\t\t\t   xctspawn(1)"
  manpageQuestion1: What is the primary purpose of the xctspawn command-line utility?
  manpageQuestion2: How can you use xctspawn to run XCTest unit and UI tests on a device while specifying a custom result bundle path?
  manpageQuestion3: Can you provide an example of using xctspawn to enumerate all tests in a test plan and output the results as a JSON file?

